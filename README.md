# Bridging the Modality Gap - Generative Adversarial Networks for T1-T2 MRI Image Translation
## A Beginner's Guide to CycleGAN and to this project


Imagine a world where getting different types of MRI scans is quick, easy, and safe. That's exactly what CycleGAN, an advanced deep learning model, is capable of doing. 


**What is CycleGAN?**

CycleGAN stands for "Cycle-Consistent Generative Adversarial Network." It's a type of artificial intelligence that can take an image in one style and convert it to another style. For example, it can transform a T1-weighted MRI scan into a T2-weighted MRI scan. This is incredibly useful in medicine.

**Why is This Important?**

Less Time, Less Stress: Typically, patients need multiple MRI scans to get a complete picture of what's happening inside their bodies. This process can be time-consuming and stressful. With CycleGAN, we can generate different types of MRI images from a single scan, saving time and reducing the hassle for patients.

Safer Diagnoses: Every time a patient undergoes an MRI, they are exposed to some level of radiation. By using CycleGAN to create necessary images from one initial scan, we can minimize radiation exposure, making the process much safer.

Vital Information: The images generated by CycleGAN are not just random pictures. They contain all the essential information doctors need to diagnose and treat patients effectively. This means no loss of crucial medical details.

Scalability and Customization: CycleGAN is highly adaptable. It can be customized for various types of medical images, not just MRIs. This flexibility means it can be applied to different imaging technologies across healthcare, improving patient care everywhere.

## Working

The process begins by loading both the T1 and T2 MRI images. After loading, the images are read and then converted
to NumPy arrays. The images are then resized to 256 X 256 pixels to ensure that all images are of uniform dimensions. The pixel values are also normalized to a range of [-1 1]. This is performed by dividing the pixel values by 127.5 and subtracting 1 from them. The
GAN consists of two main components, the generator and
the discriminator networks.



![image](https://github.com/user-attachments/assets/a4449193-4bc8-408d-8f1f-0c49638b2bfc)


 



**The Generator Network**


The generator is based on the U-net architecture. U-Net
is used because it is known to be very effective in image to
image translation. The Generator Network is primarily broken
down into the following blocks as shown in the figure below.


![image](https://github.com/user-attachments/assets/e86bbbc0-cb03-4f3d-a440-aa7171a39611)

1) Downsampling: The Downsampling block is used to
extract features while reducing the spatial dimensions of
the input images. Every Downsampling block consists of a
Convolutional layer, followed by an Instance Normalization
layer. The output from the Instance Normalisation layer is then
passed through a Leaky ReLU Activation. A Convolutional
layer serves the purpose of detecting specific patterns in
the input image and to extract low level features. An Instance
Normalization layer helps in normalizing the features to make
sure that the training is stabilized and the rate of convergence
can be accelerated. The Leaky ReLU introduces non-linearity
and helps learn complex patters and mappings between the
input and the output. Also the vanishing gradient problem is
addressed with the help of Leaky ReLU function.
2) Residual Blocks: After the Downsampling block, the
output is then sent to the Residual Blocks. The Convolutional,
Instance Normalization and the Leaky ReLU function remain
the same in the Residual Blocks. But the Residual Blocks are
followed by a second Convolutional layer and then the output
of the second Convolutional layer is finally concatenated with
the input feature map. This property is called skip connection
and helps in creating a proper gradient flow and also maintaining
training stability.
3) Upsampling: The output from the Residual Blocks are
upsampled or reconstructed to form high resolution images.
Every Upsampling block contains a Transposed Convolutional
layer, Instance Normalization layer and ReLU Activation just
like the Downsampling block. In the Transposed Convolutional
layer, the spatial dimensions of the images are expanded.
4) Output Layer: The output layer of the generator is
a convolutional layer that makes use of the tanh activation
function. The tanh activation function is used since it maps
the pixels to a range of [-1 1].



**The Discriminator Network**


The discriminator network is based on PatchGAN architecture.
It penalises the image structure at the local level.
Then it classifies the image if it is real or fake using the
NxN patch of the image. Similar to the Generator Network,
the architecture contains a downsampling block followed by
convolutional layers and the output layer. Unlike the output
layer in the Generator Network, it does not have any activation
function. Rather it represents the probability of each patch
whether it is real or fake for NXN patches. The following figure represents the Discriminator Architecture.

![image](https://github.com/user-attachments/assets/34e3af19-74f4-4748-a364-dcc906159c67)

**Loss Functions**


The loss functions that are used to ensure smooth training
are Adversarial Loss, Cycle-Consistency Loss and Identity
Loss. The Adversarial Loss function plays a vital role in
generating the image in the most genuine way possible.This
is achieved by calculating the error between the generated
image and the real image. Cycle-Consistency Loss function
is responsible for maintaining the consistency between the
translated and the original image throughout the cycle. The
content of the image is preserved throughout the translation
process. This is made sure by employing the Identity loss
function, which penalizes the differences between the input
and the translated image.
## Dataset
The dataset comprises MRI images, organized into two sub-directories,
”trainT1” and ”trainT2”. Each sub-directory contains a distinct
set of images, with 43 images in ”trainT1” and 46 images in
”trainT2”. Each image in the dataset is of size 181X217. The
dataset can be used for working on translating images from
one modality to another and testing how well they can be
translated. The figures below provide sample images from the
dataset.





![image](https://github.com/user-attachments/assets/9dee4834-7a00-4c8b-b99e-b8350b041e13)
![image](https://github.com/user-attachments/assets/41cc4f1e-a825-4412-8ff8-0e486b8f91d3)



## Results
After completing the training process of the cyclic GAN
model, significant results have surfaced, highlighting its proficiency
in image translation tasks. The following figures demonstrates
the generation of an image using random noise with
untrained generator models. It can be understood from this
figure that it exhibits inherent randomness and lacks coherence
with the target images.




![image](https://github.com/user-attachments/assets/15aaa1c7-6a32-46c7-9f4f-549b011ed326)
![image](https://github.com/user-attachments/assets/f3c60447-080b-4a0f-9374-12de468add0c)



However, as the training unfolds and the model iteratively
refines its parameters, a improvement in image translation
becomes apparent. The figures below depict the evolution of
the model’s performance across different epochs, specifically
showcasing outputs at epoch 1, epoch 100, and epoch 145 respectively.
These visualizations provide a comprehensive view of how the
model refines its translation capabilities over time, showcasing
its learning progression and improvement. The comparison in
this study helps in understanding how the model behaves in
the very initial stages and how it can advance through several
epochs and achieve a good performance.


![image](https://github.com/user-attachments/assets/eb447272-7ec1-4beb-b67e-dd095b514e22)
![image](https://github.com/user-attachments/assets/955953c0-935c-4be7-a7b7-b41c742db338)
![image](https://github.com/user-attachments/assets/b399ce06-0dd5-4654-afd1-21e6f4fe0298)



## Future Work

1. Augment the existing training dataset with supplementary MRI images from diverse sources to improve model accuracy and robustness.
2. Evaluate across different modalities and organs.
3. Test the CycleGAN model on a wider range of MRI modalities and various organs to assess its performance comprehensively and ensure its applicability in diverse clinical scenarios.
4. Integrate more sophisticated evaluation metrics to better capture the quality and clinical relevance of the generated images, ensuring the model's effectiveness and reliability in medical practice.

