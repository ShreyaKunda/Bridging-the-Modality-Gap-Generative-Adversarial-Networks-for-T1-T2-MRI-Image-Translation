# Bridging-the-Modality-Gap-Generative-Adversarial-Networks-for-T1-T2-MRI-Image-Translation
## About the project
A Beginner's Guide to CycleGAN and to this project

Imagine a world where getting different types of MRI scans is quick, easy, and safe. That's exactly what CycleGAN, an advanced deep learning model, is making possible! Let's dive into how this technology is transforming healthcare, especially in medical imaging.

What is CycleGAN?

CycleGAN stands for "Cycle-Consistent Generative Adversarial Network." It's a type of artificial intelligence that can take an image in one style and convert it to another style. For example, it can transform a T1-weighted MRI scan into a T2-weighted MRI scan. This is incredibly useful in medicine.

Why is This Important?

Less Time, Less Stress: Typically, patients need multiple MRI scans to get a complete picture of what's happening inside their bodies. This process can be time-consuming and stressful. With CycleGAN, we can generate different types of MRI images from a single scan, saving time and reducing the hassle for patients.

Safer Diagnoses: Every time a patient undergoes an MRI, they are exposed to some level of radiation. By using CycleGAN to create necessary images from one initial scan, we can minimize radiation exposure, making the process much safer.

Vital Information: The images generated by CycleGAN are not just random pictures. They contain all the essential information doctors need to diagnose and treat patients effectively. This means no loss of crucial medical details.

Scalability and Customization: CycleGAN is highly adaptable. It can be customized for various types of medical images, not just MRIs. This flexibility means it can be applied to different imaging technologies across healthcare, improving patient care everywhere.

How Does It Work?

Think of CycleGAN as a very smart artist. It looks at a T1-weighted MRI (one type of scan) and learns to paint it in the style of a T2-weighted MRI (another type of scan). This transformation is done in a way that ensures the new image is medically accurate and useful for doctors.

The Big Picture: A Safer, Faster, and Better Way to Diagnose

By integrating CycleGAN into medical imaging, we are stepping into a future where diagnosing patients is faster, safer, and more efficient. Patients get the care they need without unnecessary scans, reducing their exposure to radiation and speeding up the diagnostic process.

In essence, CycleGAN is not just a boring project; it's a groundbreaking innovation that's making healthcare better for everyone. Whether you're a beginner or an expert, understanding and appreciating the impact of this technology can open your eyes to the future of medical imaging.

## Working
![image](https://github.com/user-attachments/assets/a4449193-4bc8-408d-8f1f-0c49638b2bfc)

## Dataset
The dataset comprises MRI images, organized into two sub-directories,
”trainT1” and ”trainT2”. Each sub-directory contains a distinct
set of images, with 43 images in ”trainT1” and 46 images in
”trainT2”. Each image in the dataset is of size 181X217. The
dataset can be used for working on translating images from
one modality to another and testing how well they can be
translated. The figures below provide sample images from the
dataset.





![image](https://github.com/user-attachments/assets/9dee4834-7a00-4c8b-b99e-b8350b041e13)
![image](https://github.com/user-attachments/assets/41cc4f1e-a825-4412-8ff8-0e486b8f91d3)



## Results
After completing the training process of the cyclic GAN
model, significant results have surfaced, highlighting its proficiency
in image translation tasks. The following figures demonstrates
the generation of an image using random noise with
untrained generator models. It can be understood from this
figure that it exhibits inherent randomness and lacks coherence
with the target images.




![image](https://github.com/user-attachments/assets/15aaa1c7-6a32-46c7-9f4f-549b011ed326)
![image](https://github.com/user-attachments/assets/f3c60447-080b-4a0f-9374-12de468add0c)



However, as the training unfolds and the model iteratively
refines its parameters, a improvement in image translation
becomes apparent. The figures below depict the evolution of
the model’s performance across different epochs, specifically
showcasing outputs at epoch 1, epoch 100, and epoch 145 respectively.
These visualizations provide a comprehensive view of how the
model refines its translation capabilities over time, showcasing
its learning progression and improvement. The comparison in
this study helps in understanding how the model behaves in
the very initial stages and how it can advance through several
epochs and achieve a good performance.


![image](https://github.com/user-attachments/assets/eb447272-7ec1-4beb-b67e-dd095b514e22)
![image](https://github.com/user-attachments/assets/955953c0-935c-4be7-a7b7-b41c742db338)
![image](https://github.com/user-attachments/assets/b399ce06-0dd5-4654-afd1-21e6f4fe0298)





