# Bridging-the-Modality-Gap-Generative-Adversarial-Networks-for-T1-T2-MRI-Image-Translation
## About the project
CycleGAN, a significant deep learning model, has transformed medical imaging by providing a remedy for translating MRI images between different modalities. Its ability to transform T1-weighted to T2-weighted images is crucial in healthcare environments where obtaining multiple scans can be timeconsuming
and burdensome for patients. This can be achieved by
utilizing CycleGAN, which generates images that contains vital
information that will help the healthcare professionals to diagnose
the patient without exposing them to further radiations. This
unleashes an effective and safer way of diagnosing patients. The
adaptability of CycleGAN also allows scalability and the ability to
customize it for different types of medical images, where scanning
can be achieved with less exposure to radiation, without loosing
or compromising any important information.

## Working
![image](https://github.com/user-attachments/assets/a4449193-4bc8-408d-8f1f-0c49638b2bfc)

## Dataset
The dataset comprises MRI images, organized into two sub-directories,
”trainT1” and ”trainT2”. Each sub-directory contains a distinct
set of images, with 43 images in ”trainT1” and 46 images in
”trainT2”. Each image in the dataset is of size 181X217. The
dataset can be used for working on translating images from
one modality to another and testing how well they can be
translated. 
![image](https://github.com/user-attachments/assets/9dee4834-7a00-4c8b-b99e-b8350b041e13)
![image](https://github.com/user-attachments/assets/41cc4f1e-a825-4412-8ff8-0e486b8f91d3)


